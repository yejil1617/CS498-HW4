from pyspark.sql import SparkSession
import pandas

# Initialize Spark session
spark = SparkSession \
    .builder \
    .appName("Search Log") \
    .getOrCreate()

# Read the search.log file
file_path = "searchLog.csv"

# Parse the log file
parsed_data = []
with open(file_path, 'r') as file:
    for line in file:
        line = line.strip()
        # Split the line into search term and url:click pairs
        parts = line.split(",")
            
        term = parts[0].replace("searchTerm: ", "")
        term = term.replace("‘", "").replace("’", "")

        pairs = parts[1].split("~")
        
        for pair in pairs:
            url_click = pair.split(":")
            if len(url_click) == 2:
                url = url_click[0]
                clicks = int(url_click[1])
                parsed_data.append((term.strip(), url.strip(), clicks))


columns = ["term", "url", "clicks"] 
# Create DataFrame from the parsed data
df = spark.createDataFrame(parsed_data, columns)

# # Create output directory if it doesn't exist
# output_dir = "processed_data"
# if not os.path.exists(output_dir):
#     os.makedirs(output_dir)

# # Save DataFrame as JSON files
# df.write.json(output_dir, mode="overwrite")
df.toPandas().to_json('processed_data.json', orient='records', force_ascii=False, lines=True)

# Stop Spark session
spark.stop()

print(f"Data successfully processed and saved")

















from flask import Flask, request, jsonify
from pyspark.sql import SparkSession
from urllib.parse import urlparse
from collections import OrderedDict

app = Flask(__name__)
app.json.sort_keys = False

# Initialize Spark session
spark = SparkSession \
    .builder \
    .appName("Search Log") \
    .getOrCreate()

df = spark.read.json("processed_data.json")
df.createOrReplaceTempView("search_data")

def get_domain_priority(url):
    """Helper function to determine domain priority for sorting"""
    domain = urlparse(url).netloc
    if domain.endswith('.org'):
        return 0
    elif domain.endswith('.edu'):
        return 1
    elif domain.endswith('.com'):
        return 2
    else:
        return 3

@app.route('/', methods=['GET'])
def hello_cloud():
  return 'hello'

@app.route('/results', methods=['POST'])
def get_results():
    data = request.get_json()
    search_term = data.get('term', '')
    
    # Query for the search term
    results_df = spark.sql(f"""
        SELECT url, clicks
        FROM search_data
        WHERE term = '{search_term}'
        ORDER BY clicks DESC 
    """).collect()
    
    # Convert to list of tuples and sort in Python
    results = [(row['url'], row['clicks']) for row in results_df]
    # Sort by clicks descending, then by domain priority
    results.sort(key=lambda x: (-x[1], get_domain_priority(x[0])))

    ordered_results = OrderedDict()
    for row in results:
        ordered_results[row[0]] = row[1]
    
    return jsonify({"results": ordered_results})

@app.route('/trends', methods=['POST'])
def get_trends():
    data = request.get_json()
    search_term = data.get('term', '')

    total_clicks = spark.sql(f"""
        SELECT SUM(clicks) as total
        FROM search_data
        WHERE term = '{search_term}'
    """).collect()
    
    return jsonify({"clicks": total_clicks[0]['total']})

@app.route('/popularity', methods=['POST'])
def get_popularity():
    data = request.get_json()
    url = data.get('url', '')
    
    total_clicks = spark.sql(f"""
        SELECT SUM(clicks) as total
        FROM search_data
        WHERE url = '{url}'
    """).collect()
    
    return jsonify({"clicks": total_clicks[0]['total']})

@app.route('/getBestTerms', methods=['POST'])
def get_best_terms():
    data = request.get_json()
    website = data.get('website', '')
    
    # Get total clicks for the website
    total_website_clicks = spark.sql(f"""
        SELECT SUM(clicks) as total
        FROM search_data
        WHERE url = '{website}'
    """).collect()
    
    total_website_clicks = total_website_clicks[0]['total']
    
    if not total_website_clicks:
        return jsonify({"best_terms": []})
    
    
    best_terms_df = spark.sql(f"""
        SELECT term, SUM(clicks) as term_clicks
        FROM search_data
        WHERE url = '{website}'
        GROUP BY term
        HAVING term_clicks > {0.05 * total_website_clicks}
    """).collect()
    
    best_terms = [row['term'] for row in best_terms_df]
    
    return jsonify({"best_terms": best_terms})

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8080, debug=True)
